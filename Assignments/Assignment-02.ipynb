{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Please see the Lesson02 review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to Github and Why do we use Jupyter and Pycharm;\n",
    "Ans:Github is a place where we can share our projects to the other people;\n",
    "It includes creating project,submmit your files/codes to the project,share/contribute to other people's project etc.\n",
    "\n",
    "We use Jupyter because it has python kernel and it save the variables/functions that we wrote one by one, we don't have to run the whole piece of code at one time.\n",
    "\n",
    "Pycharm can help us debug,and it has some extra features that many text editors that don't,it may be more convenient for the person\n",
    "who needs to debug all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's the Probability Model?\n",
    "Ans: A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event.\n",
    "One of the most famous probability model is Byesian Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "Ans: 1.Filtering spams; 2.Classification on Machine Learning; 3.Whether forecasting;4.Building NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "Ans:We use probability in order to better understand the likelihood of a particular thing happening. An individual is able to better calculate such likelihoods if he has more information and context about the subject he is attempting to predict.\n",
    "In my oppinion the difficult points for programming based on parsing and pattern match is that:\n",
    "1.We may not be able to get all the context when we made the patterns for the samples we got, the new context may not match the patterns we've make for the previous samples.\n",
    "\n",
    "2.If you have a quite large text file and you want to grap many information from that, there may be too many patterns to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What's the Language Model;\n",
    "Ans:A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "1.Filter spam;2.Extract core information from text;3.Convert text data to structure data;4.Machine translation;5.Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What's the 1-gram language model\n",
    "Ans: 1-gram language model is one of the N-gram models, 1-gram model is a sequence of one item(word/character),we can use the model to assign probabilities to the entire squence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What's the disadvantages and advantages of 1-gram language model\n",
    "Ans: \n",
    "Disadvantage:When we use 1-gram model, we seperate the sentences by the characters one by one, which means we no longer have the exactlly meaning of this character, then the probabilities we get from 1-gram model may not be helpful.\n",
    "\n",
    "Advantage: It's the simplest model among the N-gram models,it's easier to caculate the probabilities of every character when we don't have to know the accurate meaning of the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What't the 2-gram models\n",
    "Ans: \n",
    "2-gram model is one of the N-gram model too,it's also called bigram, it is a sequence of 2 items. Two items will be combined together so that we know the relationship of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. what's the web crawler, and can you implement a simple crawler?\n",
    "Ans: Web crawler is a script or a program made for getting the content from the webpages/APIs.\n",
    "Yes I do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?\n",
    "Ans: \n",
    "<p>1.Some webpages contain javascript tools, and we can't get any massage if we use the traditional cralwing tools,but we can use the tools that can handle javascript such as webkit and selenium.\n",
    "<p>2.Some webpages have anti-cralwing stratigies like blocking the IP address that acting unnormal, we can use IP proxy to avoid that\n",
    "<p>3.Some webpages ask us to login before we visit the information on that page, that will be a difficult story..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. What't the Regular Expression and how to use?\n",
    "Ans: \n",
    "<p>In theoretical computer science and formal language theory, a regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations.\n",
    "I refer to this website when I use them:\n",
    "<p>![链接](http://www.runoob.com/python/python-reg-expressions.html)  #Why markdown link is not working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) How to solve OOV problem?\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this out-of-vocabulary(OOV) problems. There are so many intelligent man to solve this probelm.\n",
    "\n",
    "--\n",
    "\n",
    "The first question is:\n",
    "\n",
    "Q1: How did you solve this problem in your programming task?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Then, the sencond question is:\n",
    "\n",
    "Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task\n",
    "\n",
    "Reference:\n",
    "\n",
    "https://www.wikiwand.com/en/Good%E2%80%93Turing_frequency_estimation\n",
    "https://github.com/Computing-Intelligence/References/blob/master/NLP/Natural-Language-Processing.pdf, Page-37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: You need to download the corpus from wikipedis:\n",
    "\n",
    "https://dumps.wikimedia.org/zhwiki/20190401/\n",
    "\n",
    "Step 2: You may need the help of wiki-extractor:\n",
    "\n",
    "https://github.com/attardi/wikiextractor\n",
    "\n",
    "Step 3: Using the technologies and methods to finish the language model;\n",
    "\n",
    "Step 4: Try some interested sentence pairs, and check if your model could fit them\n",
    "\n",
    "Step 5: If we need to solve following problems, how can language model help us?\n",
    "\n",
    "Voice Recognization.\n",
    "Sogou pinyin input.\n",
    "Auto correction in search engine.\n",
    "Abnormal Detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "import os  \n",
    "from hanziconv import HanziConv \n",
    "from collections import Counter \n",
    "\n",
    "path = '/Users/tracy/Downloads/资料/NLP作业/AA'\n",
    "\n",
    "pathes = os.listdir(path)\n",
    "\n",
    "pathes = [os.path.join(path,x) for x in pathes]\n",
    "pathes.sort()\n",
    "\n",
    "new_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pathes[:5]:\n",
    "    with open(p,'r',encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        text = text.split('\\n')\n",
    "        text = [s for s in text if s]\n",
    "        for t in text:\n",
    "            if '<' in t and '>' and ('id=' in t or 'url=' in t):\n",
    "                pass\n",
    "            else:    \n",
    "                new_text.append(t.replace('</doc>',''))\n",
    "\n",
    "new_text = [s for s in new_text if s]\n",
    "del text \n",
    "\n",
    "new_text = ' '.join(new_text)\n",
    "new_text = HanziConv.toSimplified(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wy/v0gb2_ds3716l6fjs3x1nvkc0000gn/T/jieba.cache\n",
      "Loading model cost 0.621 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "valid_tokens = jieba.lcut(new_text)\n",
    "del new_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拆分后的词语总共数量: 5172626\n"
     ]
    }
   ],
   "source": [
    "#get frequency of all the words\n",
    "word_count = Counter(valid_tokens)\n",
    "\n",
    "all_frequencies = [f for w,f in word_count.most_common()]\n",
    "\n",
    "frequencies_sum = len(valid_tokens)\n",
    "\n",
    "print('拆分后的词语总共数量:',frequencies_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007639419404125286\n"
     ]
    }
   ],
   "source": [
    "def get_prob(word):\n",
    "    esp = 1 / frequencies_sum\n",
    "    if word in word_count:\n",
    "        return 1/word_count[word]\n",
    "    else:\n",
    "        return esp \n",
    "\n",
    "print(get_prob('我们'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = jieba.lcut(string)\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_gram_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "\n",
    "_2_gram_sum = len(all_2_gram_words)\n",
    "_2_gram_counter = Counter(all_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.799764297578946e-07\n"
     ]
    }
   ],
   "source": [
    "def get_combination_prob(w1,w2):\n",
    "    if w1 + w2 in _2_gram_counter:\n",
    "        return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum\n",
    "\n",
    "a = get_combination_prob('去','北京')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00024571668074076133\n"
     ]
    }
   ],
   "source": [
    "def get_prob_2_gram(w1,w2):\n",
    "    return get_combination_prob(w1,w2) / get_prob(w1)\n",
    "\n",
    "a = get_prob_2_gram('去','沈阳')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.561052824939101e-35\n"
     ]
    }
   ],
   "source": [
    "def language_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    words = jieba.lcut(sentence)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous,word)\n",
    "        sentence_probability *=  prob \n",
    "    return sentence_probability\n",
    "\n",
    "a = language_model_of_2_gram('小明今天抽奖抽到一台波音飞机')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase \n",
    "noun_phrase => Article Adj* noun belong \n",
    "belong => de property\n",
    "de => 的\n",
    "property => 眼睛 | 裙子 | 胳膊 | 尾巴\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\"\n",
    "\n",
    "def parse_grammar(grammar_str, sep='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'): \n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        target, rules = line.split(sep)\n",
    "        \n",
    "        grammar[target.strip()] = [r.split() for r in rules.split('|')]\n",
    "    \n",
    "    return grammar\n",
    "\n",
    "\n",
    "def gene(grammar_parsed, target='sentence'):\n",
    "    if target not in grammar_parsed: return target\n",
    "    \n",
    "    rule = random.choice(grammar_parsed[target])\n",
    "    return ''.join(gene(grammar_parsed, target=r) for r in rule if r!='null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = parse_grammar(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_generated = [gene(g) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一个女人的胳膊看见一个女人的眼睛',\n",
       " '这个篮球的眼睛看见一个女人的裙子',\n",
       " '一个女人的尾巴听着一个篮球的眼睛',\n",
       " '这个小猫的眼睛坐在这个女人的眼睛',\n",
       " '一个小猫的眼睛听着一个女人的胳膊',\n",
       " '这个女人的眼睛坐在一个小猫的胳膊',\n",
       " '这个女人的尾巴坐在一个篮球的尾巴',\n",
       " '这个女人的眼睛看着这个小猫的尾巴',\n",
       " '这个女人的眼睛看着一个桌子的裙子',\n",
       " '这个女人的裙子坐在一个篮球的眼睛',\n",
       " '一个小猫的胳膊坐在一个女人的尾巴',\n",
       " '这个桌子的裙子看见一个女人的尾巴',\n",
       " '这个女人的胳膊坐在这个小猫的眼睛',\n",
       " '这个女人的胳膊看见这个篮球的胳膊',\n",
       " '这个小猫的胳膊听着一个女人的裙子',\n",
       " '一个篮球的胳膊看见这个篮球的尾巴',\n",
       " '一个桌子的尾巴看见这个篮球的胳膊',\n",
       " '一个小猫的胳膊听着这个女人的眼睛',\n",
       " '一个桌子的胳膊坐在这个女人的尾巴',\n",
       " '一个篮球的裙子坐在一个篮球的尾巴',\n",
       " '这个小猫的眼睛听着这个篮球的裙子',\n",
       " '这个桌子的裙子看见这个篮球的眼睛',\n",
       " '这个女人的胳膊看着这个桌子的尾巴',\n",
       " '这个桌子的裙子坐在一个小猫的眼睛',\n",
       " '这个桌子的尾巴听着一个小猫的裙子',\n",
       " '一个桌子的胳膊坐在一个小猫的胳膊',\n",
       " '这个篮球的裙子看着一个小猫的裙子',\n",
       " '一个桌子的胳膊听着一个桌子的胳膊',\n",
       " '一个桌子的裙子听着这个篮球的尾巴',\n",
       " '一个好看的女人的尾巴听着这个女人的眼睛',\n",
       " '这个桌子的胳膊看着这个桌子的裙子',\n",
       " '一个篮球的眼睛看着这个小小的女人的裙子',\n",
       " '这个蓝色的篮球的尾巴坐在一个篮球的胳膊',\n",
       " '一个桌子的胳膊听着这个小猫的裙子',\n",
       " '一个小小的女人的裙子看见一个桌子的眼睛',\n",
       " '这个小小的女人的胳膊坐在一个篮球的尾巴',\n",
       " '这个蓝色的桌子的尾巴看着这个篮球的尾巴',\n",
       " '这个小小的篮球的尾巴看着一个篮球的胳膊',\n",
       " '这个女人的尾巴听着这个小小的篮球的胳膊',\n",
       " '这个小小的桌子的胳膊坐在一个女人的裙子',\n",
       " '一个女人的胳膊听着一个小小的蓝色的女人的眼睛',\n",
       " '一个小小的桌子的尾巴坐在一个桌子的眼睛',\n",
       " '一个小猫的胳膊看着一个蓝色的篮球的裙子',\n",
       " '一个篮球的眼睛坐在这个好看的桌子的胳膊',\n",
       " '一个桌子的裙子看着一个蓝色的小猫的裙子',\n",
       " '一个小小的篮球的裙子看着一个小猫的尾巴',\n",
       " '一个桌子的尾巴听着这个蓝色的桌子的胳膊',\n",
       " '一个小猫的裙子看着这个小小的篮球的尾巴',\n",
       " '这个蓝色的蓝色的篮球的裙子看着这个女人的眼睛',\n",
       " '这个好看的篮球的尾巴坐在这个小猫的裙子',\n",
       " '这个桌子的胳膊坐在这个好看的蓝色的女人的眼睛',\n",
       " '一个小小的蓝色的篮球的裙子看见一个小猫的眼睛',\n",
       " '一个好看的女人的裙子坐在一个蓝色的蓝色的女人的眼睛',\n",
       " '这个好看的篮球的胳膊看着这个蓝色的女人的胳膊',\n",
       " '这个篮球的尾巴看见一个好看的小小的小猫的眼睛',\n",
       " '一个女人的胳膊看见一个好看的小小的桌子的尾巴',\n",
       " '这个好看的蓝色的女人的眼睛坐在一个小小的篮球的尾巴',\n",
       " '这个蓝色的小小的桌子的胳膊看见一个桌子的胳膊',\n",
       " '这个好看的好看的篮球的胳膊看着一个女人的眼睛',\n",
       " '这个蓝色的好看的篮球的眼睛听着这个小猫的裙子',\n",
       " '这个桌子的裙子听着这个小小的蓝色的篮球的尾巴',\n",
       " '这个小小的小猫的胳膊看见这个小小的桌子的胳膊',\n",
       " '这个小小的蓝色的蓝色的女人的胳膊看见一个蓝色的小猫的眼睛',\n",
       " '一个小猫的裙子听着一个好看的好看的小猫的尾巴',\n",
       " '这个好看的小小的蓝色的小猫的眼睛看着一个桌子的眼睛',\n",
       " '这个小小的好看的女人的眼睛看着一个小小的小猫的裙子',\n",
       " '一个好看的好看的蓝色的小猫的眼睛看着这个篮球的尾巴',\n",
       " '这个小小的蓝色的篮球的眼睛看着这个小小的小猫的裙子',\n",
       " '这个小小的好看的篮球的裙子坐在这个蓝色的女人的胳膊',\n",
       " '这个蓝色的好看的小小的桌子的裙子看见一个女人的尾巴',\n",
       " '一个蓝色的小猫的眼睛听着一个好看的蓝色的蓝色的桌子的眼睛',\n",
       " '这个好看的桌子的裙子看见这个好看的篮球的裙子',\n",
       " '一个好看的小猫的眼睛坐在一个小小的好看的小猫的尾巴',\n",
       " '这个小小的好看的桌子的胳膊听着这个小猫的裙子',\n",
       " '一个小小的好看的小小的小猫的眼睛看见一个小猫的胳膊',\n",
       " '一个女人的胳膊坐在一个好看的好看的蓝色的桌子的尾巴',\n",
       " '这个蓝色的篮球的胳膊看着一个蓝色的好看的桌子的眼睛',\n",
       " '这个小小的桌子的眼睛看见一个好看的小小的篮球的裙子',\n",
       " '这个小猫的尾巴听着这个小小的好看的蓝色的桌子的胳膊',\n",
       " '这个好看的小猫的裙子看着这个蓝色的小小的篮球的胳膊',\n",
       " '一个好看的女人的尾巴听着一个好看的好看的桌子的尾巴',\n",
       " '一个小小的小小的好看的小小的篮球的尾巴看见一个女人的眼睛',\n",
       " '一个小猫的尾巴看着这个小小的好看的好看的蓝色的女人的眼睛',\n",
       " '这个小小的蓝色的好看的女人的裙子坐在这个蓝色的篮球的胳膊',\n",
       " '一个小小的好看的好看的蓝色的小猫的眼睛听着一个篮球的尾巴',\n",
       " '这个小猫的胳膊看见一个好看的小小的好看的桌子的裙子',\n",
       " '这个好看的好看的篮球的胳膊看着一个小小的桌子的胳膊',\n",
       " '这个女人的裙子坐在一个好看的好看的蓝色的蓝色的蓝色的女人的胳膊',\n",
       " '这个小小的小小的蓝色的小小的桌子的尾巴听着这个小猫的眼睛',\n",
       " '这个小小的蓝色的桌子的胳膊坐在这个小小的蓝色的篮球的裙子',\n",
       " '这个小小的蓝色的女人的裙子听着这个好看的蓝色的小小的女人的眼睛',\n",
       " '一个好看的小小的桌子的裙子听着一个蓝色的小小的桌子的尾巴',\n",
       " '这个小小的好看的蓝色的小小的小小的篮球的裙子看见这个篮球的眼睛',\n",
       " '这个小小的小小的好看的好看的蓝色的小猫的裙子坐在一个女人的胳膊',\n",
       " '一个好看的小小的蓝色的好看的小猫的胳膊看着这个小小的女人的尾巴',\n",
       " '一个小小的小小的篮球的胳膊看着这个小小的蓝色的好看的篮球的尾巴',\n",
       " '一个女人的裙子坐在一个小小的小小的蓝色的蓝色的蓝色的蓝色的小小的桌子的裙子',\n",
       " '这个好看的小小的桌子的裙子看见这个小小的好看的好看的篮球的尾巴',\n",
       " '这个好看的小小的好看的蓝色的小猫的尾巴看见一个小小的好看的篮球的胳膊',\n",
       " '这个好看的好看的好看的蓝色的小小的女人的眼睛看着这个小小的小猫的裙子']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(random_generated, key=language_model_of_2_gram, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帕斯卡定义了气压单位 is more possible\n",
      "---- 帕斯卡定义了气压单位 with probility 1.4560209008540463e-16\n",
      "---- 帕斯卡定义了人是会思想的苇草 with probility 2.5523553728900813e-17\n",
      "中山陵在北京 is more possible\n",
      "---- 中山陵在南京 with probility 2.2004916310725398e-07\n",
      "---- 中山陵在北京 with probility 4.826884868159119e-07\n",
      "英国脱欧失败 is more possible\n",
      "---- 英国脱欧成功 with probility 4.11122138869095e-13\n",
      "---- 英国脱欧失败 with probility 4.11122138869095e-13\n",
      "特斯拉登上月球 is more possible\n",
      "---- 特斯拉登上月球 with probility 9.19418601470885e-12\n",
      "---- 特斯拉登上火星 with probility 3.0647286715696166e-12\n",
      "托尔斯泰是俄国文学的卓越代表 is more possible\n",
      "---- 陀思妥耶夫斯基是俄国文学的卓越代表 with probility 1.7247035048097422e-24\n",
      "---- 托尔斯泰是俄国文学的卓越代表 with probility 5.947495161241994e-19\n",
      "牛顿是伟大的物理学家 is more possible\n",
      "---- 牛顿是伟大的物理学家 with probility 4.628359592998301e-10\n",
      "---- 爱因斯坦是伟大的物理学家 with probility 6.611942275711857e-11\n",
      "海德堡坐落于内卡河畔 is more possible\n",
      "---- 曼海姆位于萊茵河上游河谷的北部 with probility 7.304458448524748e-19\n",
      "---- 海德堡坐落于内卡河畔 with probility 1.270741647837755e-12\n",
      "康德出生于柯尼斯堡 is more possible\n",
      "---- 欧拉解决了柯尼斯堡七桥问题 with probility 3.174601171011482e-21\n",
      "---- 康德出生于柯尼斯堡 with probility 8.555438994160594e-13\n"
     ]
    }
   ],
   "source": [
    "# more sentence pairs\n",
    "\n",
    "need_compared = [\n",
    "    \"帕斯卡定义了气压单位 帕斯卡定义了人是会思想的苇草\",\n",
    "    \"中山陵在南京 中山陵在北京\",\n",
    "    \"英国脱欧成功 英国脱欧失败\",\n",
    "    \"特斯拉登上月球 特斯拉登上火星\",\n",
    "    \"陀思妥耶夫斯基是俄国文学的卓越代表 托尔斯泰是俄国文学的卓越代表\",\n",
    "    \"牛顿是伟大的物理学家 爱因斯坦是伟大的物理学家\",\n",
    "    \"曼海姆位于萊茵河上游河谷的北部 海德堡坐落于内卡河畔\",\n",
    "    \"欧拉解决了柯尼斯堡七桥问题 康德出生于柯尼斯堡\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = language_model_of_2_gram(s1), language_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
