{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review the course online programming code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Please see the Lesson02 review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review the main points of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How to Github and Why do we use Jupyter and Pycharm;\n",
    "Ans:Github is a place where we can share our projects to the other people;\n",
    "It includes creating project,submmit your files/codes to the project,share/contribute to other people's project etc.\n",
    "\n",
    "We use Jupyter because it has python kernel and it save the variables/functions that we wrote one by one, we don't have to run the whole piece of code at one time.\n",
    "\n",
    "Pycharm can help us debug,and it has some extra features that many text editors that don't,it may be more convenient for the person\n",
    "who needs to debug all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's the Probability Model?\n",
    "Ans: A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event.\n",
    "One of the most famous probability model is Byesian Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "Ans: 1.Filtering spams; 2.Classification on Machine Learning; 3.Whether forecasting;4.Building NLP models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "Ans:We use probability in order to better understand the likelihood of a particular thing happening. An individual is able to better calculate such likelihoods if he has more information and context about the subject he is attempting to predict.\n",
    "In my oppinion the difficult points for programming based on parsing and pattern match is that:\n",
    "1.We may not be able to get all the context when we made the patterns for the samples we got, the new context may not match the patterns we've make for the previous samples.\n",
    "\n",
    "2.If you have a quite large text file and you want to grap many information from that, there may be too many patterns to write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What's the Language Model;\n",
    "Ans:A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length m, it assigns a probability to the whole sequence. Having a way to estimate the relative likelihood of different phrases is useful in many natural language processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "1.Filter spam;2.Extract core information from text;3.Convert text data to structure data;4.Machine translation;5.Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What's the 1-gram language model\n",
    "Ans: 1-gram language model is one of the N-gram models, 1-gram model is a sequence of one item(word/character),we can use the model to assign probabilities to the entire squence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What's the disadvantages and advantages of 1-gram language model\n",
    "Ans: \n",
    "Disadvantage:When we use 1-gram model, we seperate the sentences by the characters one by one, which means we no longer have the exactlly meaning of this character, then the probabilities we get from 1-gram model may not be helpful.\n",
    "\n",
    "Advantage: It's the simplest model among the N-gram models,it's easier to caculate the probabilities of every character when we don't have to know the accurate meaning of the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What't the 2-gram models\n",
    "Ans: \n",
    "2-gram model is one of the N-gram model too,it's also called bigram, it is a sequence of 2 items. Two items will be combined together so that we know the relationship of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. what's the web crawler, and can you implement a simple crawler?\n",
    "Ans: Web crawler is a script or a program made for getting the content from the webpages/APIs.\n",
    "Yes I do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?\n",
    "Ans: \n",
    "<p>1.Some webpages contain javascript tools, and we can't get any massage if we use the traditional cralwing tools,but we can use the tools that can handle javascript such as webkit and selenium.\n",
    "<p>2.Some webpages have anti-cralwing stratigies like blocking the IP address that acting unnormal, we can use IP proxy to avoid that\n",
    "<p>3.Some webpages ask us to login before we visit the information on that page, that will be a difficult story..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. What't the Regular Expression and how to use?\n",
    "Ans: \n",
    "<p>In theoretical computer science and formal language theory, a regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations.\n",
    "I refer to this website when I use them:\n",
    "<p>![链接](http://www.runoob.com/python/python-reg-expressions.html)  #Why markdown link is not working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using Wikipedia dataset to finish the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba \n",
    "import os  \n",
    "from hanziconv import HanziConv \n",
    "from collections import Counter \n",
    "\n",
    "path = '/Users/tracy/Downloads/资料/NLP作业/AA'\n",
    "\n",
    "pathes = os.listdir(path)\n",
    "\n",
    "pathes = [os.path.join(path,x) for x in pathes]\n",
    "pathes.sort()\n",
    "\n",
    "new_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in pathes[:50]:\n",
    "    with open(p,'r',encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        text = text.split('\\n')\n",
    "        text = [s for s in text if s]\n",
    "        for t in text:\n",
    "            if '<' in t and '>' and ('id=' in t or 'url=' in t):\n",
    "                pass\n",
    "            else:    \n",
    "                new_text.append(t.replace('</doc>',''))\n",
    "\n",
    "new_text = [s for s in new_text if s]\n",
    "del text \n",
    "\n",
    "new_text = ' '.join(new_text)\n",
    "new_text = HanziConv.toSimplified(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/wy/v0gb2_ds3716l6fjs3x1nvkc0000gn/T/jieba.cache\n",
      "Loading model cost 0.640 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "valid_tokens = jieba.lcut(new_text)\n",
    "del new_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "拆分后的词语总共数量: 51863083\n"
     ]
    }
   ],
   "source": [
    "#get frequency of all the words\n",
    "word_count = Counter(valid_tokens)\n",
    "\n",
    "all_frequencies = [f for w,f in word_count.most_common()]\n",
    "\n",
    "frequencies_sum = len(valid_tokens)\n",
    "\n",
    "print('拆分后的词语总共数量:',frequencies_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.922674694977024e-05\n"
     ]
    }
   ],
   "source": [
    "def get_prob(word):\n",
    "    esp = 1 / frequencies_sum\n",
    "    if word in word_count:\n",
    "        return 1/word_count[word]\n",
    "    else:\n",
    "        return esp \n",
    "\n",
    "print(get_prob('我们'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = jieba.lcut(string)\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_gram_words = [''.join(valid_tokens[i:i+2]) for i in range(len(valid_tokens[:-2]))]\n",
    "\n",
    "_2_gram_sum = len(all_2_gram_words)\n",
    "_2_gram_counter = Counter(all_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.398830817629211e-07\n"
     ]
    }
   ],
   "source": [
    "def get_combination_prob(w1,w2):\n",
    "    if w1 + w2 in _2_gram_counter:\n",
    "        return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum\n",
    "\n",
    "a = get_combination_prob('去','北京')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002628652162026394\n"
     ]
    }
   ],
   "source": [
    "def get_prob_2_gram(w1,w2):\n",
    "    return get_combination_prob(w1,w2) / get_prob(w1)\n",
    "\n",
    "a = get_prob_2_gram('去','沈阳')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.944481737517095e-33\n"
     ]
    }
   ],
   "source": [
    "def language_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    words = jieba.lcut(sentence)\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous,word)\n",
    "        sentence_probability *=  prob \n",
    "    return sentence_probability\n",
    "\n",
    "a = language_model_of_2_gram('小明今天抽奖抽到一台波音飞机')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase \n",
    "noun_phrase => Article Adj* noun belong \n",
    "belong => de property\n",
    "de => 的\n",
    "property => 眼睛 | 裙子 | 胳膊 | 尾巴\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\"\n",
    "\n",
    "def parse_grammar(grammar_str, sep='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'): \n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        target, rules = line.split(sep)\n",
    "        \n",
    "        grammar[target.strip()] = [r.split() for r in rules.split('|')]\n",
    "    \n",
    "    return grammar\n",
    "\n",
    "\n",
    "def gene(grammar_parsed, target='sentence'):\n",
    "    if target not in grammar_parsed: return target\n",
    "    \n",
    "    rule = random.choice(grammar_parsed[target])\n",
    "    return ''.join(gene(grammar_parsed, target=r) for r in rule if r!='null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = parse_grammar(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_generated = [gene(g) for _ in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这个女人的眼睛坐在一个女人的尾巴',\n",
       " '这个女人的尾巴看见这个女人的眼睛',\n",
       " '这个篮球的眼睛听着一个女人的眼睛',\n",
       " '一个篮球的尾巴看见一个女人的尾巴',\n",
       " '一个女人的裙子听着这个女人的眼睛',\n",
       " '一个篮球的尾巴听着这个女人的尾巴',\n",
       " '一个蓝色的小猫的眼睛看见一个女人的胳膊',\n",
       " '一个女人的眼睛听着这个桌子的裙子',\n",
       " '这个女人的尾巴听着一个小猫的裙子',\n",
       " '一个桌子的尾巴看着一个女人的眼睛',\n",
       " '一个蓝色的小猫的眼睛看见这个女人的裙子',\n",
       " '一个女人的尾巴坐在这个桌子的尾巴',\n",
       " '一个女人的眼睛看着这个小猫的裙子',\n",
       " '一个女人的尾巴看着这个小猫的尾巴',\n",
       " '一个桌子的尾巴坐在这个女人的眼睛',\n",
       " '一个蓝色的女人的胳膊看见这个蓝色的女人的尾巴',\n",
       " '这个女人的眼睛看着这个小猫的胳膊',\n",
       " '一个蓝色的女人的裙子看着这个篮球的眼睛',\n",
       " '一个小小的蓝色的女人的眼睛坐在一个女人的胳膊',\n",
       " '一个桌子的眼睛看见一个桌子的裙子',\n",
       " '一个好看的小猫的尾巴看见一个女人的眼睛',\n",
       " '一个小小的女人的尾巴看着一个女人的裙子',\n",
       " '一个小猫的胳膊听着这个女人的胳膊',\n",
       " '这个桌子的眼睛看见这个桌子的胳膊',\n",
       " '一个小小的篮球的尾巴看见这个女人的胳膊',\n",
       " '这个女人的胳膊看着这个桌子的尾巴',\n",
       " '一个桌子的眼睛坐在这个小猫的尾巴',\n",
       " '一个桌子的眼睛坐在一个桌子的胳膊',\n",
       " '一个篮球的裙子看着一个蓝色的蓝色的篮球的眼睛',\n",
       " '这个蓝色的蓝色的桌子的胳膊听着一个女人的尾巴',\n",
       " '这个好看的女人的胳膊坐在一个篮球的眼睛',\n",
       " '这个蓝色的女人的眼睛坐在这个小小的女人的尾巴',\n",
       " '这个桌子的尾巴听着这个桌子的裙子',\n",
       " '这个蓝色的篮球的裙子听着一个小猫的裙子',\n",
       " '一个好看的篮球的眼睛听着这个小猫的眼睛',\n",
       " '一个桌子的尾巴看着一个小猫的胳膊',\n",
       " '这个篮球的裙子看着这个桌子的裙子',\n",
       " '这个蓝色的女人的胳膊看见这个好看的女人的裙子',\n",
       " '一个桌子的尾巴看见这个小小的女人的尾巴',\n",
       " '这个女人的胳膊看着这个好看的篮球的眼睛',\n",
       " '这个小小的女人的胳膊看见一个篮球的裙子',\n",
       " '一个好看的女人的眼睛看见一个好看的篮球的胳膊',\n",
       " '一个桌子的胳膊听着这个蓝色的篮球的尾巴',\n",
       " '一个小猫的裙子看见这个好看的女人的胳膊',\n",
       " '一个篮球的胳膊看见一个小小的小猫的眼睛',\n",
       " '这个篮球的裙子坐在一个好看的蓝色的女人的胳膊',\n",
       " '这个桌子的胳膊看着这个篮球的胳膊',\n",
       " '一个蓝色的蓝色的篮球的裙子看见一个小小的篮球的尾巴',\n",
       " '一个小小的篮球的尾巴看见这个小猫的胳膊',\n",
       " '一个好看的蓝色的篮球的尾巴坐在一个桌子的眼睛',\n",
       " '一个蓝色的女人的尾巴看见一个蓝色的蓝色的好看的篮球的胳膊',\n",
       " '一个桌子的眼睛看见一个蓝色的小小的蓝色的女人的眼睛',\n",
       " '一个女人的尾巴看见这个蓝色的小小的篮球的胳膊',\n",
       " '这个小小的女人的胳膊看见这个小猫的尾巴',\n",
       " '这个桌子的胳膊看着这个蓝色的篮球的裙子',\n",
       " '一个桌子的裙子看着这个好看的篮球的眼睛',\n",
       " '一个桌子的裙子听着一个小小的小猫的裙子',\n",
       " '一个桌子的尾巴看见一个小小的蓝色的桌子的尾巴',\n",
       " '一个好看的蓝色的蓝色的桌子的眼睛看见这个小猫的尾巴',\n",
       " '一个小小的桌子的尾巴看着一个小猫的胳膊',\n",
       " '这个桌子的尾巴坐在这个小小的桌子的尾巴',\n",
       " '一个小小的女人的裙子看着一个蓝色的蓝色的桌子的尾巴',\n",
       " '一个蓝色的蓝色的好看的桌子的胳膊听着这个女人的眼睛',\n",
       " '一个桌子的裙子看见这个好看的桌子的胳膊',\n",
       " '一个小小的蓝色的桌子的裙子听着这个篮球的裙子',\n",
       " '这个小小的女人的尾巴看着一个小小的小猫的尾巴',\n",
       " '这个篮球的尾巴坐在这个好看的好看的女人的裙子',\n",
       " '一个小小的蓝色的桌子的眼睛坐在一个好看的篮球的眼睛',\n",
       " '这个女人的眼睛坐在这个好看的小小的小猫的尾巴',\n",
       " '一个篮球的尾巴看见一个小小的好看的好看的女人的眼睛',\n",
       " '一个好看的桌子的尾巴坐在一个小小的小猫的眼睛',\n",
       " '这个蓝色的好看的女人的尾巴坐在一个小小的桌子的尾巴',\n",
       " '这个篮球的尾巴听着这个蓝色的蓝色的好看的桌子的裙子',\n",
       " '一个小小的好看的女人的尾巴坐在这个蓝色的桌子的裙子',\n",
       " '一个蓝色的好看的好看的蓝色的蓝色的女人的尾巴听着一个篮球的尾巴',\n",
       " '这个篮球的裙子坐在这个好看的好看的篮球的眼睛',\n",
       " '一个桌子的胳膊坐在这个蓝色的小小的篮球的眼睛',\n",
       " '这个小小的篮球的胳膊坐在这个小小的桌子的眼睛',\n",
       " '这个女人的胳膊看见一个好看的好看的好看的女人的胳膊',\n",
       " '一个篮球的眼睛看见这个蓝色的好看的小小的小猫的裙子',\n",
       " '一个好看的小小的桌子的眼睛听着这个蓝色的小猫的尾巴',\n",
       " '一个小小的好看的篮球的尾巴看着这个小小的小猫的胳膊',\n",
       " '一个小小的好看的女人的尾巴坐在这个蓝色的小小的小猫的胳膊',\n",
       " '一个小小的好看的小小的小猫的胳膊听着一个小猫的尾巴',\n",
       " '这个蓝色的好看的蓝色的好看的篮球的尾巴听着一个好看的蓝色的篮球的尾巴',\n",
       " '一个小小的小小的篮球的尾巴看着一个小小的小小的篮球的眼睛',\n",
       " '这个蓝色的小小的小猫的裙子看着这个小小的桌子的胳膊',\n",
       " '一个小小的篮球的胳膊看着这个好看的小小的小猫的尾巴',\n",
       " '一个好看的蓝色的好看的蓝色的好看的女人的胳膊听着这个小猫的胳膊',\n",
       " '一个桌子的裙子听着这个小小的小小的好看的蓝色的桌子的胳膊',\n",
       " '这个好看的蓝色的蓝色的蓝色的小小的蓝色的篮球的尾巴听着一个小小的蓝色的小猫的眼睛',\n",
       " '一个小小的好看的小小的篮球的眼睛看着一个小小的小小的篮球的裙子',\n",
       " '这个女人的胳膊听着一个好看的好看的小小的小小的蓝色的蓝色的小猫的眼睛',\n",
       " '一个好看的小小的好看的好看的蓝色的桌子的尾巴看见这个蓝色的桌子的胳膊',\n",
       " '一个好看的好看的好看的桌子的尾巴听着这个小小的小小的篮球的裙子',\n",
       " '一个小小的蓝色的好看的好看的蓝色的桌子的尾巴坐在一个蓝色的小小的桌子的尾巴',\n",
       " '这个好看的蓝色的小小的蓝色的好看的小小的篮球的眼睛坐在这个桌子的裙子',\n",
       " '这个好看的小小的好看的蓝色的好看的小猫的裙子看见一个好看的好看的蓝色的女人的尾巴',\n",
       " '这个好看的篮球的尾巴看着一个小小的小小的小小的蓝色的好看的小小的小猫的胳膊',\n",
       " '一个好看的好看的小小的好看的好看的蓝色的小猫的裙子看着一个好看的桌子的眼睛']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(random_generated, key=language_model_of_2_gram, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
